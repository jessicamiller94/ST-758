{
    "contents" : "\n\n# performance\n\n# microbenchmarking\ninstall.packages(\"microbenchmark\")\nlibrary(\"microbenchmark\")\n?microbenchmark\n\n# benchmark the square root\nx <- 1+runif(100)\nmicrobenchmark(sqrt(x), x^(1/2), x^0.5 )\nsv <- microbenchmark(sqrt(x), x^(1/2), x^0.5, times = 1000 )\n\n# look at the median because there can be huge outliers\nplot(sv$time)\n\n# square root taylor series\nsqrt_newton <- function(x,niter){\n    y <- 1.2\n    for(j in 1:niter){\n        y <- 0.5*(y + x/y)\n    }\n    y\n}\nplot(sqrt_newton(x,6)-sqrt(x))\n\n# time our new function, spoiler alert: we lose :( \nsqrt_newton_6 <- function(x) sqrt_newton(x,6)\nx <- runif(100)\nmicrobenchmark(sqrt(x), x^(0.5), sqrt_newton(x,6), sqrt_newton_6(x), exp(log(x)/2))\n\n\n\n# R is has dynamic typing, so when expressions are \n# evaluated, R does not know ahead of time what type of\n# objects it is operating on. Strongly typed languages\n# (C, C++, Java) force programmers to specify the type.\n# this allows the compiler to pick the most efficient\n# method for evaluating the expression\n\n# these two take the same amount of time, but\n# in theory the second could be faster\nsystem.time({ \n    x <- 0L\n    for(x in 1:1e6) x <- x+1L\n})\ntypeof(x)\n\nsystem.time({ \n    x <- 0\n    for(x in 1:1e6) x <- x+1\n})\ntypeof(x)\n\n# this is even slower\nsystem.time({\n    x <- 0\n    for(x in 1:1e6) x <- x[1]+1\n})\n\n\n# it's actually worse than that because R does not \n# appear to have specific methods for adding integers\nx1 <- rep(1L,100)\nx2 <- rep(1 ,100)\ntypeof(sum(x1))\ntypeof(sum(x2))\nmicrobenchmark( sum( x1 ), sum( x2 ) )\n\n# still MUCH faster than writing the loop\nloopsum <- function(x){\n    s <- 0\n    for(j in seq_along(x)) s <- s+x[j]\n    s\n}\nmicrobenchmark( sum( x1 ), loopsum(x1) )\n\n\n\n# cost of method dispatch is different for different\n# OO systems\n\n# function to be evaluated (just returns NULL)\nf <- function(x) NULL\n\n# s3 generic\ns3 <- function(x) UseMethod(\"s3\")\ns3.integer <- f\n\n# s4 generic\nA <- setClass(\"A\", representation(a = \"list\"))\nsetGeneric(\"s4\", function(x) standardGeneric(\"s4\"))\nsetMethod(s4, \"A\", f)\n\n# Reference Class (we didn't cover this OO system)\nB <- setRefClass(\"B\", methods = list(rc = f))\n\n# generate instances (nothing to do for s3)\na <- A()\nb <- B$new()\n\n# no method dispatch is fastest, but of the three OO\n# systems, s3 appears to be fastest.\nmicrobenchmark(\n    fun = f(),\n    S3 = s3(1L),\n    S4 = s4(a),\n    RC = b$rc(), times = 1000\n)\n\n\n\n# Because R is so dynamic, it has to look through\n# the search path each time it accesses a variable\n# in the following example, \"a\" is found in a different\n# environment each time print(a) is called\na <- 1\nf <- function() {\n    g <- function() {\n        print(a)\n        assign(\"a\", 2, envir = parent.frame())\n        print(a)\n        a <- 3\n        print(a)\n    }\n    g()\n}\nf()\n\n# This can cause performance issues\n# even (, [, {, are functions\nf1 <- function(x, y) {\n    (x + y) ^ 2\n}\nf2 <- function(x, y) {\n    (((((((( x + y )))))))) ^ 2\n}\nmicrobenchmark( f1(1.2,3.4),\n                f2(1.2,3.4), \n                times = 1000 )\n\n\n\n\n# the following examples shows how searching through\n# environments can slow down the function evaluation\nf <- f1\nrandom_env <- function(parent = globalenv()) {\n    letter_list <- setNames(as.list(runif(26)), LETTERS)\n    list2env(letter_list, envir = new.env(parent = parent))\n}\nset_env <- function(f, e) {\n    environment(f) <- e\n    f\n}\nf2 <- set_env(f, random_env())\nf3 <- set_env(f, random_env(environment(f2)))\nf4 <- set_env(f, random_env(environment(f3)))\n\nenvironment(f)\nparent.env(environment(f2))\nparent.env(parent.env(environment(f3)))\nparent.env(parent.env(parent.env(environment(f4))))\n\nmicrobenchmark(\n    f(1, 2),\n    f2(1, 2),\n    f3(1, 2),\n    f4(1, 2),\n    times = 10000\n)\n\n\n# having additional function arguments causes some overhead\n# slowdown because, even though R does not evaluate\n# the arguments until called upon (lazy evaluation)\n# it still creates an object (called a promise) that\n# contains the expressions\nf0 <- function() NULL\nf1 <- function(a = 1) NULL\nf2 <- function(a = 1, b = 1) NULL\nf3 <- function(a = 1, b = 2, c = 3) NULL\nf4 <- function(a = 1, b = 2, c = 4, d = 4) NULL\nf5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL\nmicrobenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times = 10000)\n\n\n\n\n# the previous issues are associated with the way that\n# the R language works: no integers, \"(\" and \"[\" are\n# functions, environment structure, lazy evaluation.\n\n# these things necessarily cause slowdown because they\n# are part of the core R structure\n\n# there are other aspects of R that are simply poorly\n# implemented. Subsetting data frames is one of them\nmtcars\ndim(mtcars)\n\n# pull out the bottom right corner of mtcars\nmicrobenchmark(\n    \"[32, 11]\"      = mtcars[32, 11],\n    \"$carb[32]\"     = mtcars$carb[32],\n    \"[[c(11, 32)]]\" = mtcars[[c(11, 32)]],\n    \"[[11]][32]\"    = mtcars[[11]][32],\n    \".subset2\"      = .subset2(mtcars, 11)[32]\n)\n# [32,11] is ridiculously slow compared to .subset2!\n\n# this is even true for matrices (to a lesser extent)\nmtmat <- as.matrix(mtcars)\nmicrobenchmark(\n    \"[32, 11]\"      = mtmat[32, 11],\n    \"[[11]][32]\"    = mtmat[,11][32],\n    \".subset\"      = .subset(mtmat, 11*32)\n)\n# .subset is twice as fast as [32,11]\n\nmtmat <- matrix(runif(1e6),1e3,1e3)\nmicrobenchmark(\n    \"[32, 11]\"      = mtmat[1000, 1000],\n    \"[[11]][32]\"    = mtmat[,1000][1000],\n    \".subset\"      = .subset(mtmat, 1e6),\n    \"linear indexing\" = mtmat[1e6]\n)\n\n\n\n\n\n# implementation of ifelse, pmin, pmax are slow\nsquish_ife <- function(x, a, b) {\n    ifelse(x <= a, a, ifelse(x >= b, b, x))\n}\nsquish_p <- function(x, a, b) {\n    pmax(pmin(x, b), a)\n}\nsquish_in_place <- function(x, a, b) {\n    x[x <= a] <- a\n    x[x >= b] <- b\n    x\n}\n\n# compare them\nx <- runif(100, -1.5, 1.5)\nmicrobenchmark(\n    squish_ife      = squish_ife(x, -1, 1),\n    squish_p        = squish_p(x, -1, 1),\n    squish_in_place = squish_in_place(x, -1, 1),\n    unit = \"us\"\n)\n\n\n# There is no reason for comparing logicals to be \n# slower than adding doubles\nx1 <- sample(c(TRUE,FALSE),20, replace= TRUE)\nx2 <- sample(c(TRUE,FALSE),20, replace= TRUE)\ny1 <- runif(20)\ny2 <- runif(20)\nmicrobenchmark( x1 | x2, y1 + y2 )\n\n\n# R does have a built-in compiler \nlibrary(compiler)\nf <- function(n, x) for (i in 1:n) x = (1 + x)^(-1)\ng <- cmpfun(f)\n\n# still not as fast as avoiding the loop\nmicrobenchmark(f(1000, 1), \n               g(1000, 1), \n               (1+rep(1,1000))^(-1),\n               1/(1+rep(1,1000)),\n               times = 1000)\n\n\n\n\n\n\n\n\n",
    "created" : 1481066774728.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1272019261",
    "id" : "4FA71F0E",
    "lastKnownWriteTime" : 1474943742,
    "path" : "F:/ST 758/jsguinneST758_2016/performance.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}